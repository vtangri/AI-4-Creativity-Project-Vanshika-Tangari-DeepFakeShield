"""
Report generation Celery worker tasks.
"""
from pathlib import Path
from typing import Dict, Any
from datetime import datetime
import json

from celery import shared_task

from app.core.celery_app import celery_app, TaskState
from app.core.config import settings
from app.db.session import SessionLocal
from app.models import AnalysisJob, Report


def update_job_status(job_id: str, stage: str, progress: float, error: str = None):
    """Update job status in database."""
    db = SessionLocal()
    try:
        job = db.query(AnalysisJob).filter(AnalysisJob.id == job_id).first()
        if job:
            job.stage = stage
            job.status = stage
            job.progress = progress
            if error:
                job.error_message = error
                job.status = TaskState.FAILED
            db.commit()
    finally:
        db.close()


# LLM System Prompt for Report Generation
REPORT_SYSTEM_PROMPT = """You are DeepFakeShield Report Assistant.
You write forensic-style summaries of an AI deepfake analysis.

You must:
- Use cautious language (likely, suggests, indicates)
- Highlight evidence and timestamps
- Include limitations and next verification steps

You must NOT:
- Provide instructions to create deepfakes
- Explain how to bypass detection
- Give advice on evading moderation

Output format:
1. Summary verdict with confidence
2. Evidence highlights with timestamps
3. Modality breakdown
4. Limitations
5. Recommended verification steps
"""


def generate_report_with_llm(analysis_results: Dict[str, Any]) -> str:
    """Generate report using LLM (placeholder implementation)."""
    
    # Check if OpenAI API key is configured
    if not settings.OPENAI_API_KEY:
        return _generate_fallback_report(analysis_results)
    
    try:
        import openai
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        response = client.chat.completions.create(
            model=settings.LLM_MODEL,
            messages=[
                {"role": "system", "content": REPORT_SYSTEM_PROMPT},
                {"role": "user", "content": f"Generate a forensic report for this analysis:\n{json.dumps(analysis_results, indent=2)}"}
            ],
            max_tokens=1000,
            temperature=0.3,
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return _generate_fallback_report(analysis_results)


def _generate_fallback_report(results: Dict[str, Any]) -> str:
    """Generate a fallback report without LLM."""
    overall_score = results.get("overall_score", 0)
    label = results.get("label", "UNKNOWN")
    video_score = results.get("video_score", 0)
    audio_score = results.get("audio_score", 0)
    lipsync_score = results.get("lipsync_score", 0)
    
    # Determine verdict text
    if label == "AUTHENTIC":
        verdict = "This media appears to be authentic."
        confidence = "high"
    elif label == "LIKELY_FAKE":
        verdict = "This media shows some indicators of potential manipulation."
        confidence = "medium"
    else:
        verdict = "This media shows strong indicators of manipulation."
        confidence = "high"
    
    report = f"""# DeepFakeShield Analysis Report

## Summary Verdict

{verdict}

**Overall Suspicion Score:** {overall_score:.1%}
**Classification:** {label}
**Confidence:** {confidence}

## Modality Analysis

### Video Analysis
- **Score:** {video_score:.1%}
- Analyzed video frames for manipulation artifacts
- {"No significant anomalies detected" if video_score < 0.5 else "Potential manipulation indicators found"}

### Audio Analysis  
- **Score:** {audio_score:.1%}
- Analyzed audio for synthetic speech markers
- {"Audio appears authentic" if audio_score < 0.5 else "Audio shows potential synthesis patterns"}

### Lip-Sync Analysis
- **Score:** {lipsync_score:.1%}
- Verified audio-visual synchronization
- {"Lip movements align with audio" if lipsync_score < 0.5 else "Potential lip-sync mismatch detected"}

## Limitations

⚠️ **Important considerations:**

1. AI detection is not 100% accurate
2. Results should be considered alongside other evidence
3. Detection performance may vary with compression and quality
4. Novel deepfake techniques may not be detected

## Recommended Next Steps

1. **Verify the source**: Check the original publication source
2. **Request original file**: Compressed versions may affect analysis
3. **Cross-reference**: Compare with known authentic media from the same source
4. **Consult experts**: For high-stakes decisions, consider forensic expert review

---
*Generated by DeepFakeShield AI on {datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")}*
"""
    
    return report


@celery_app.task(bind=True, queue="default", max_retries=3)
def generate_report(self, job_id: str, fusion_results: Dict[str, Any]) -> Dict[str, Any]:
    """Generate the final forensic report."""
    try:
        update_job_status(job_id, TaskState.REPORT, 0.0)
        
        # Generate report text
        report_text = generate_report_with_llm(fusion_results)
        
        update_job_status(job_id, TaskState.REPORT, 0.5)
        
        # Create full report JSON
        full_report = {
            "version": "1.0.0",
            "job_id": job_id,
            "generated_at": datetime.utcnow().isoformat(),
            "verdict": {
                "label": fusion_results.get("label"),
                "overall_score": fusion_results.get("overall_score"),
            },
            "analysis": {
                "video_score": fusion_results.get("video_score"),
                "audio_score": fusion_results.get("audio_score"),
                "lipsync_score": fusion_results.get("lipsync_score"),
            },
            "report_text": report_text,
        }
        
        # Save report to database
        db = SessionLocal()
        try:
            report = db.query(Report).filter(Report.job_id == job_id).first()
            
            if not report:
                report = Report(job_id=job_id)
                db.add(report)
            
            report.summary = report_text
            report.full_report = full_report
            report.generated_at = datetime.utcnow()
            report.llm_model_used = settings.LLM_MODEL if settings.OPENAI_API_KEY else "fallback"
            
            db.commit()
        finally:
            db.close()
        
        update_job_status(job_id, TaskState.REPORT, 1.0)
        
        return {
            "job_id": job_id,
            "report_generated": True,
        }
        
    except Exception as e:
        update_job_status(job_id, TaskState.FAILED, 0.0, str(e))
        raise


@celery_app.task(bind=True, queue="default")
def finalize_job(self, job_id: str, report_result: Dict[str, Any]) -> Dict[str, Any]:
    """Finalize the analysis job."""
    try:
        from datetime import datetime
        
        db = SessionLocal()
        try:
            job = db.query(AnalysisJob).filter(AnalysisJob.id == job_id).first()
            if job:
                job.status = TaskState.DONE
                job.stage = TaskState.DONE
                job.progress = 1.0
                job.completed_at = datetime.utcnow()
                db.commit()
        finally:
            db.close()
        
        return {
            "job_id": job_id,
            "status": TaskState.DONE,
        }
        
    except Exception as e:
        update_job_status(job_id, TaskState.FAILED, 0.0, str(e))
        raise


@celery_app.task(bind=True, queue="default")
def run_full_pipeline(self, job_id: str):
    """Run the complete analysis pipeline."""
    from app.workers.preprocess import run_preprocessing_pipeline
    from app.workers.inference import run_inference_pipeline
    
    try:
        # Run preprocessing
        preprocess_results = run_preprocessing_pipeline.apply(args=[job_id]).get()
        
        # Run inference
        inference_results = run_inference_pipeline.apply(
            args=[job_id, preprocess_results]
        ).get()
        
        # Generate report
        report_result = generate_report.apply(args=[job_id, inference_results]).get()
        
        # Finalize
        final_result = finalize_job.apply(args=[job_id, report_result]).get()
        
        return final_result
        
    except Exception as e:
        update_job_status(job_id, TaskState.FAILED, 0.0, str(e))
        raise
